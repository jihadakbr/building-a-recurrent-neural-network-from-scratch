
# Building a Recurrent Neural Network from Scratch

This project was completed as a part of the Honors portion of the [Sequence Models](https://www.coursera.org/learn/nlp-sequence-models) Course on [Coursera](https://www.coursera.org/).

Credit to DeepLearning.AI and the Coursera platform for providing the course materials and guidance.

## Objective

In this assignment, the main objective is to implement crucial components of a Recurrent Neural Network (RNN) using the NumPy library. By doing so, I will gain a deep understanding of how RNNs work and their applications in various tasks, particularly in Natural Language Processing and other sequence-related challenges.

Throughout the assignment, I will achieve several key goals. Firstly, I will define the necessary notation for building sequence models, setting the foundation for further exploration of RNNs. Secondly, I will delve into the architecture of a basic RNN, comprehending its structure and functionality in sequence processing tasks.

Moreover, I will focus on understanding the core elements of a Long Short-Term Memory (LSTM) network, a more sophisticated RNN variant known for handling long-range dependencies and overcoming the vanishing gradient problem.
## Results

![Backward Recurrent Neural Network](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiN-lcd_RyfQEY6GDN4SNWNHdzJngJWaki25gHmClgW1y5OZA5Oe_WMqpi9uJ3Av2k37unqNi8M-NSXc_RO2KZU3FPN2VuzAmmVbvwceSE3kx1x4W7TCS-QFthbF70qKza1YOATy8B58sjXXvXCIBZ-Rs1Vl51lcDqJG649kSr7YFzC7dLAx7BxIbF_nbA/s1600/backward-recurrent-neural-network.png)